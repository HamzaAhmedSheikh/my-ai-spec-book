---
title: "Module 4 Overview: Vision-Language-Action (VLA)"
sidebar_label: "Overview"
sidebar_position: 19
---

# Module 4: Vision-Language-Action (VLA)

## Overview

This module focuses on the convergence of Large Language Models (LLMs) and Robotics. You'll learn to build robots that understand natural language commands and execute complex tasks.

This module will guide you through:

1. **Chapter 16**: Introduction to VLA - LLMs meet Robotics
2. **Chapter 17**: Voice-to-Action - OpenAI Whisper for voice commands
3. **Chapter 18**: Cognitive Planning - LLMs for task planning
4. **Chapter 19**: Natural Language to ROS Actions - Translating commands to robot actions
5. **Chapter 20**: Capstone Project - The Autonomous Humanoid

---

## Learning Objectives

By the end of this module, you will be able to:

- Integrate voice recognition with robot control
- Use LLMs to translate natural language to robot actions
- Build cognitive planning systems for robots
- Create an autonomous humanoid that responds to voice commands
- Complete a full-stack Physical AI project

---

## Prerequisites

- Completion of Module 1 (ROS 2)
- Completion of Module 2 (Gazebo & Unity)
- Completion of Module 3 (NVIDIA Isaac)
- Basic understanding of LLMs and NLP
- Python programming experience

---

## Next Steps

Begin with [Chapter 16: Introduction to VLA](./chapter-1-introduction) to explore the future of human-robot interaction.

